# Summary

* [Introduction](README.md)
* [1 Why Machine Learning Strategy](chapter1.md)
* [2 How to use this book to help your team](chapter2.md)
* [3 Prerequisites and Notation](chapter3.md)
* [4 Scale drives machine learning progress](chapter4.md)
* [5 Your development and test sets](chapter5.md)
* [6 Your dev and test sets should come from the same distribution](chapter6.md)
* [7 How large do the dev/test sets need to be?](chapter7.md)
* [8 Establish a single-number evaluation metric for your team to optimize](chapter8.md)
* [9 Optimizing and satisficing metrics](chapter9.md)
* [10 Having a dev set and metric speeds up iterations](chapter10.md)
* [11 When to change dev/test sets and metrics](chapter11.md)
* [12 Takeaways: Setting up development and test sets](chapter12.md)
* [13 Build your first system quickly, then iterate](chapter13.md)
* [14 Error analysis: Look at dev set examples to evaluate ideas](chapter14.md)
* [15 Evaluate multiple ideas in parallel during error analysis](chapter15.md)
* [16 Cleaning up mislabeled dev and test set examples](chapter16.md)
* [17 If you have a large dev set, split it into two subsets, only one of which you look at](chapter17.md)
* [18 How big should the Eyeball and Blackbox dev sets be?](chapter18.md)
* [19 Takeaways: Basic error analysis](chapter19.md)
* [20 Bias and Variance: The two big sources of error](chapter20.md)
* [21 Examples of Bias and Variance](chapter21.md)
* [22 Comparing to the optimal error rate](chapter22.md)
* [23 Addressing Bias and Variance](chapter23.md)
* [24 Bias vs. Variance tradeoff](chapter24.md)
* [25 Techniques for reducing avoidable bias](chapter25.md)
* [26 Error analysis on the training set](chapter26.md)
* [27 Techniques for reducing variance](chapter27.md)
* [28 Diagnosing bias and variance: Learning curves](chapter28.md)
* [29 Plotting training error](chapter29.md)
* [30 Interpreting learning curves: High bias](chapter30.md)
* [31 Interpreting learning curves: Other cases](chapter31.md)
* [32 Plotting learning curves](chapter32.md)
* [33 Why we compare to human-level performance](chapter33.md)
* [34 How to define human-level performance](chapter34.md)
* [35 Surpassing human-level performance](chapter35.md)
* [36 When you should train and test on different distributions](chapter36.md)
* [37 How to decide whether to use all your data](chapter37.md)
* [38 How to decide whether to include inconsistent data](chapter38.md)
* [39 Weighting data](chapter39.md)
* [40 Generalizing from the training set to the dev set](chapter40.md)
* [41 Identifying Bias, Variance, and Data Mismatch Errors](chapter41.md)
* [42 Addressing data mismatch](chapter42.md)
* [43 Artificial data synthesis](chapter43.md)
* [44 The Optimization Verification test](chapter44.md)
* [45 General form of Optimization Verification test](chapter45.md)
* [46 Reinforcement learning example](chapter46.md)
* [47 The rise of end-to-end learning](chapter47.md)
* [48 More end-to-end learning examples](chapter48.md)
* [49 Pros and cons of end-to-end learning](chapter49.md)
* [50 Choosing pipeline components: Data availability](chapter50.md)
* [51 Choosing pipeline components: Task simplicity](chapter51.md)
* [52 Directly learning rich outputs](chapter52.md)


