## Chapter 22、Comparing to the optimal error rate

**比较最优错误率**

在我们猫识别的例子中，理想错误率——就是，一个“最优”分类器可达到的——接近0%。如果图片中有猫，人几乎总是可以识别出来。因此，我们希望可以做的一样好的机器。

其他问题更难。例如，假设你正在构建一个语音识别系统，并发现14%的音频片段有太多的背景噪声，或者很难理解，甚至人都不能识别出所说的内容。在这种情况下，即使是“最优”语音识别系统也可能有约为14%的错误。

假设在语音识别问题上，你的算法达到：

- 训练错误率 = 15%
- 开发错误率 = 30%

训练集上的表现以及接近最优的错误率14%。因此，在偏差上或者在训练集的表现上没有太多提升的空间。然而，算法没有很好的泛化到开发集上；因此在由于方差而导致的错误上还有很大的提升空间。

这个例子和前一章第三个例子类似，都有15%的训练错误率和30%的开发错误率。如果最优的错误率是 ~0%，那么15%的训练错误率留下了很大的提升空间。这表明减少偏差的变化可能是有益的。但是，如果最优错误率是14%，那么同样的训练集上的表现告诉我们，在分类器的偏差几乎没有改进的余地。

对于最佳错误率远不为零的问题，这里有一个对算法错误更详细的分解。继续上面我们语音识别的示例，可以将总的30%的开发集错误分解如下（类似的分析可以应用于测试集错误）：

- **最优错误率（“不可避免的偏差”）**：14%。假设我们决定，即使是世界上最好的语音系统，我们仍会遭受14%的错误。我们可以将其认为是学习算法的偏差“不可避免”的部分。
- **可避免的偏差**：1%。通过训练错误率和最优误差率之间的差值来计算【3】。
- **方差**：15%。开发错误和训练错误之间的差值。

为了将这与我们之前的定义联系起来，偏差和可避免的偏差关系如下【4】：

偏差 = 最佳误差率（“不可避免偏差”）+ 可避免的偏差

这个“可避免的偏差”反映了算法在训练集上的表现比“最优分类器”差多少。

方差的概念和之前保持一致。理论上来说，我们可以通过训练一个大规模训练集总是可以减少方差接近于零。因此，拥有足够大数据集，所有的方差都是可以“避免的”，所以不存在所谓的“不可避免的方差”。

再考虑一个例子，该例子中最优错误率是14%，我们有：

- 训练错误 = 15%
- 开发错误 = 16%

然而在前一章中我们称之为高偏差分类器，现在我们说可避免偏差的误差是1%，方差的误差约为1%。因此，算法已经做的很好了，几乎没有提升的空间。它只比最佳错误率差2%。

从这些例子中我们可以看出，了解最优错误率有利于指导我们的后续步骤。在统计学上，最优错误率也被成为**贝叶斯错误率（Bayes error rate）**，或贝叶斯率。

我们如何才能知道最优错误率是多少呢？对于人类还算擅长的任务，例如识别图片或转录音频剪辑，你可以要求人们提供标签，然后测量人为标签相对于你训练集的准确率。这将给出最优错误率的估计。如果你正在解决甚至人也很难解决的问题（例如预测推荐什么电影，或向用户展示什么广告），这将很难估计最优错误率。

在“与人类表现比较”（第33~35章）这一节中，我将更详细的讨论学习算法的表现和人类表现相比较的过程。

在最后几章中，你学习了如何通过查看训练集和卡法鸡错误率来估计可避免/不可避免的偏差和方差。下一章将讨论如何使用这种分析的见解来优先考虑减少偏差还是减少方差的技术。根据你项目当前的问题是高（可避免的）偏差还是高方差，你应该应用非常不同的技术。请继续阅读。



————————

【3】如果该值是负的，你在训练集上的表现比最优错误率要好。这意味着你正在过拟合训练集，并且算法已经过度记忆（over-memorized）训练集。你应该专注于方差减少的方法，而不是进一步减少偏差的方法。

【4】选择这些定义是为了表达如何改进学习算法的见解。这些定义与统计学家定义偏差和方差不同。从技术上来说，我这里定义的“偏差”应该被叫做“我们认为是偏差的错误”；以及“可避免的偏差”应该被称为“我们认为学习算法的偏差超过最优错误率的错误”。