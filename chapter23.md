## Chapter 23、Addressing Bias and Variance

**处理偏差和方差**

以下是处理偏差和方差问题最简单的公式：

- 如果具有较高的可避免偏差，那么增加模型的大小（例如，通过添加层/神经元来增加神经网络的大小）。
- 如果具有较高的方差，那么增加训练数据集。

如果你可以增加神经网络的大小，并无限制的增加训练集数据，那么可以在很多学习问题上都做的很好。

在实践中，增加网络的模型终将导致你会遇到计算问题，因为训练大的模型很慢。你也可能会耗尽获取更多训练数据的能力。（即使在网上，也只有有限数量的猫图片）

不同的模型架构（例如，不同的神经网络架构）对于你的问题将有不同的偏差/方差量。最近很多深度学习研究已经开发出很多新的模型架构。所以，如果你在使用神经网络，学术文献可能会是一个很好的灵感来源。github上也有很多好的开源实现。但是尝试新架构的结果要比增加模型大小和添加数据这一简单公式难以预测。

增加模型的大小通常可以减少偏差，但也可能会增加方差和过拟合的风险。然而，这种过拟合的问题通常只在你不使用正则化的时候出现。如果你包含了一个精心设计的正则化方法，那么你通常可以安全的增加模型的大小，而不会增加过拟合。

假设你正在应用深度学习，有L2正则化和dropout，有在开发集上表现最好的正则化参数。如果你增加模型的大小，通常你的表现会保持不变或提升；它不太可能明显的变差。避免使用更大模型的唯一原因就是计算代价变大。