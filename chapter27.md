## Chapter 27、Techniques for reducing variance

**减少方差的方法**

如果你的学习算法遭受高方差，你可以尝试以下方法：

- **添加更多训练数据**：只要你能够获取更多的数据和处理这些数据的充足计算能力，这是处理方差问题最简单也是最可靠的方法。
- **添加正则化**（L2正则化，L1正则化，dropout）：该方法减少了方差，但增加了偏差。
- **添加提前停止（early stopping）**（基于开发集错误提前停止梯度下降）：该方法减少方差但增加了偏差。提前停止的行为很像正则化方法，一些作者称它为正则化方法。


- **选择特征以减少输入特征的数目/类型：**该方法可能有助于解决方差问题，但也可能增加偏差。略微减少特征数量（比如从1000个特征减少到900）不太可能对偏差产生很大影响。显著地减少它（比如从1000减少到100——减少10倍）更可能有显著的影响，只要你没有将太多有用的特征排除在外。在现代深度学习中，当数据丰富时，已经从特征选择转移了出来，现在我们更有可能给算法我们所有的特征，并让算法根据数据分类使用哪些特征。但是当你训练集比较小时，特征选择可能非常有用。

减少模型大小（如神经元/层的数量）：谨慎使用。该方法能够减少方差，但可能增加偏差。但是，我不推荐使用该方法来处理方差。增加正则化通常能得到更好的分类性能。减少模型大小的好处就是降低计算成本，从而加快训练模型的速度。如果加快模型的训练是有用的，那无论如何考虑减少模型的大小。但是如果你的目标是减少方差，并且你不关心计算成本，那么考虑添加正则化替代之。

这里有两个额外的策略，重复上一章处理偏差中的方法：

- **基于错误分析的洞察修改输入特征**：假设错误分析启发你去创建额外的特征，以帮助算法消除特定类别的错误。这些新特征可能有助于减少偏差和方差。理论上来说，增加更多的特征可能会增加方差，但如果你发现这种情况，那么就使用正则化方法，它通常能够消除方差的增加。
- **修改模型架构**（如神经网络架构）以便更适合你的问题：这种方法能够影响偏差和方差。