## Chapter 29、Plotting training error

**绘制训练错误曲线**

你的开发集（和测试集）错误应该随着训练集大小的增长而减少。但随着训练集大小的增加，训练集错误通常会增加。

让我们举例说明这个效果。假设你的训练集有只有两个样本：一张猫图和一张非猫图。学习算法很容易“记住”训练集中这两个样本，并且训练集错误率为0%。即使有一张或两张样本图片都被错误标注，算法仍然很容易记住这两个标签。

现在假设你的训练集有100个样本。可能有一些样本是被错误标记或模棱两可的——一些图非常模糊，甚至人都不能区分是否有猫。或许学习算法仍能“记住”大部分或所有的训练集，但现在很难获得100%的准确率。通过将训练集样本数从2增加到100，你将发现训练集准确率将略有下降。

最后，假设你的训练集有10000个样本。这种情况下算法更难以完全适应10000个样本，特别是有一些样本是模棱两可或错误标注的。因此，你的学习算法在该训练集上将做的更糟。

让我们为之前的曲线（开发错误曲线）添加训练错误曲线：

![0](http://oow6unnib.bkt.clouddn.com/myl-c29-0.jpg)

你可以看到蓝色的“训练错误”曲线随着训练集大小的增长而增长。而且，算法通常在训练集上表现比在开发集上要好。因此，红色的开发错误曲线通常严格地在蓝色训练错误曲线上方。

下一步我们将讨论如何解释这些曲线。