## Chapter 30、Interpreting learning curves: High bias

**解读学习曲线：高偏差**

假设你的开发错误曲线如下图所示：

![0](http://oow6unnib.bkt.clouddn.com/myl-c30-0.jpg)

我们之前说过，如果你的开发错误曲线平稳，仅仅通过添加数据你不可能达到期望的性能。

但是很难确切的知道红色开发错误曲线的外推是什么样的。如果开发集很小，可能会更不确定，因为曲线可能很嘈杂。

假设我们在图片中添加训练错误曲线，如下所示：

![0](http://oow6unnib.bkt.clouddn.com/myl-c30-1.jpg)

现在，你可以绝对肯定的说，添加更多的数据本身并不足够。这是为什么？记住我们的两个观察结果：

- 随着我们添加更多的训练数据，训练错误只会变得更差。因此，蓝色的训练错误曲线只会保持不动或变得更高，所以它只会远离期望的性能水平（绿色的线）。
- 红色的开发错误曲线通常要高于蓝色的训练错误曲线。因此，即使训练错误高于期望性能水平，通过添加更多数据来让红色开发错误曲线下降到期望性能水平之下也基本没有可能。

在相同的图中检查开发错误曲线和训练错误曲线可以让我们更加自信地推断开发错误曲线。

为了讨论，假设期望性能是我们对最优错误率的估计。那上面的图片就是一个标准的“教科书”式的例子（具有高可避免偏差的学习曲线是什么样的）：在训练集大小的最大处（大概对应我们的所有训练数据），在训练错误和期望性能之间有大的间隙，表明大的可避免偏差。此外，训练和开发曲线之间的间隙小，表明方差小。

之前，我们只在曲线最右端的点去衡量训练集错误和开发集错误，这对应使用所有的可训练数据。绘制完整的学习曲线给我们提供了更全面图片，绘制了算法在不同训练集大小上的表现。