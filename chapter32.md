## Chapter 32、Plotting learning curves

**绘制学习曲线**

假设你有一个非常小的训练集，只有100个样本。随机选择10个样本的子集来训练你的算法，然后是20个样本，然后30，直到100，以10个为间隔增加样本数。然后使用这10个数据点绘制学习曲线。你可能会发现曲线在较小的训练集大小下看起来有些嘈杂（意思是这些值比期望的要高/低）。

当只在10个随机选择的样本上训练时，你可能不幸选到了特别“bad”的训练集，例如有很多模棱两可/错误标注的样本。或者，你可能幸运的选到了特别“good”的训练集。小的训练集意味着开发和训练错误可能会随机波动。

如果你的机器学习应用严重偏倚一个类别（如负样本远比正样本多的猫分类任务），或者类别数比较大（如识别100种不同动物类别），那么选择尤其是“不具代表性”或坏的训练集的几率更大。例如，如果80%的样本是负样本（y=0），只有20%是正样本（y=1），那么有可能10个样本的训练集只包含负样本，因此很难让算法学习到有意义的东西。

如果训练曲线里的噪声使得很难看见真实的趋势，这里有两种解决方法：

- 不是仅对10个样本的一个模型进行训练，而是通过从原始100个样本的数据集中通过替换的抽样方法【1】选择几个（如3-10）不同的随机选择的10个样本的训练集。在这些数据集上训练不同的模型，并对每个结果模型计算训练集和开发集错误。计算并绘制平均训练错误和平均开发集错误。
- 如果你的训练集比较倾向一种类别，或有很多类别，从100个训练样本中选择一个“平衡的”子集而不是随机选择的10个训练样本。例如，你可以确保2/10的样本是正样本，8/10为负样本。更为一般的说，你可以确保每个类别的样本部分尽可能的接近原始训练集的整体部分。

我不会为这些技术而烦恼，除非你已经尝试过绘制学习曲线，并且认为曲线太嘈杂以至于看不到潜在的趋势。如果你的训练集较大（比如说超过1000个样本），并且你的类别分布不是很偏，你可能不需要这些技巧。

最后，绘制学习曲线可能花费很高的计算成本：例如，你可能需要训练10个模型，其中分别有1000个样本，然后是2000个，直到10000个。使用小的数据集训练模型比使用大数据集来训练模型要快的多。因此，不像上面那样均匀地将训练集大小按线性范围划分出来，你可以在1000、2000、4000、6000和10000个样本上训练模型。这样应该仍然可以让你清晰的了解学习曲线的趋势。当然，只有在训练所有附加模型的计算成本显著时，该方法才有意义。

————————————

【1】以下是采用替换的抽样方法（sampling with replacement）：你可以从100个里面随机选择10个不同的样本作为第一个训练集。然后你会再选10个样本，形成第二个训练集，不考虑第一个训练集中选了哪些样本。因此，一个特定的样本可能出现在第一和第二个训练集中。相反，如果你的样本没有被替换，第二个训练集将仅在第一次未被选择的90个样本中进行选择。在实践中，有或没有替换的抽样不应该产生很大的差异，但前者是常见的做法。