## Chapter 44、The Optimization Verification test

**优化验证测试**

假设你在构建一个语音识别系统。系统通过输入一个音频剪辑A，并对每个可能的输出语句S计算某个`Score_A(S)`来工作。例如，给定输入音频A，你可能尝试去估计`Score_A(S) = P(S|A)`，即正确输出转录语句是S的概率。

给定计算`Score_A(S)`的方法，你仍然需要去寻找最大化它的英语语句S：

![44](http://oow6unnib.bkt.clouddn.com/myl-c44-0.jpg)

你如何计算上面的“arg max”？如果英语语言有5w个词汇，那么长度为N的语句就有(5w)^N种可能——太多而无法详尽列举。所以，你需要应用一个近似搜索算法，试图找出优化（最大化）`Score_A(S)`的值S。一个搜索算法的例子是“束搜索(beam search)”，其在搜索的过程中保持只保留K个最高候选人（就本章而言，你不需要了解束搜索的细节）。像这种算法不能保证找到最大化`Score_A(S)`的值S。

假设一个音频剪辑A记录有人说“我爱机器学习”。但你的系统不是正确的转录输出，而是输出错误的“我爱机器人”。有两种出错的可能性：

1. **搜索算法问题**。近似的搜索算法（束算法）未能寻找到最大化`Score_A(S)`的值S。
2. **目标（打分函数）问题**。我们对`Score_A(S) = P(S|A)`的估计不准确。特别地，我们选择的`Score_A(S)`没能识别出“我爱机器学习”是的正确的转录。

根据这些中的哪个是失败的原因，你应该以不同的方式优先考虑你的努力。如果#1是问题，你应该致力于提升搜索算法。如果#2是问题，你应该致力于估计`Score_A(S)`的学习算法。

面对这种情况，一些研究者将随机决定使用搜索算法；另一些研究者将随机以更好的方式去学习`Score_A(S)`的值。但除非你知道哪个是错误的根本原因，否则你的努力可能会被浪费。你怎样更系统的决定要做什么？

让S_out表示输出转录（“我爱机器人”）。让`S*`表示正确的转录（“我爱机器学习”）。为了了解上面的#1和#2哪个是问题，你可以执行**优化验证测试**：首先，计算Score_A(`S*`)和Score_A(S_out)。然后检查是否Score_A(`S*`) > Score_A(S_out)。有两种可能：

Case 1：Score_A(`S*`) > Score_A(S_out)

在这种情况下，你的学习算法已经正确地给出`S*`比S_out更高的分数。尽管如此，我们的近似搜索算法选择了S_out，而不是`S*`。这告诉你近似搜索算法未能选择最大化`Score_A(S)`的值S。在这种情况下，优化验证测试告诉你你的搜索算法有问题，你应该专注在该问题上。例如，你可以尝试增加束算法的束宽。

Case 2：Score_A(`S*`) <= Score_A(S_out)

在这种情况下，你了解到计算`Score_A(.)`的方式不对：相比错误的输出S_out，它并没有给正确的输出`S*`严格更高的分数。优化验证测试告诉你你的目标（打分）函数有问题。因此，你应该专注于提高对不同语句S的学习或近似`Score_A(S)`。

我们的讨论集中在一个例子上。为了在实践中应用优化验证测试，你应该检测开发集中的错误。对每个错误，你可以测试是否Score_A(`S*`) > Score_A(S_out)。该不等式所持有的每个开发样例将标记为由优化算法引起的错误。不等式Score_A(`S*`) <= Score_A(S_out)所持有的每个样例由于计算Score_A(.)的方式不对而被视为错误。

例如，假设你发现95%的错误是由打分函数Score_A(.)引起的，只有5%的错误是因为优化算法。现在你知道不论你改进了多少优化程序，你只会真实地消除约5%的错误。因此，你应该专注于改进评估Score_A(.)的方式。



